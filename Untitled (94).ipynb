{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4375205d-de40-4204-84fb-7dca8bd918fa",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905a88a-7bcc-486e-9968-f94b86a5bd78",
   "metadata": {},
   "source": [
    "In machine learning, an ensemble technique is a method that combines the predictions of multiple individual models to produce a more robust and accurate prediction than any of the individual models alone. The idea behind ensemble methods is to leverage the diversity among the individual models to improve overall performance, especially in cases where a single model might struggle.\n",
    "\n",
    "Ensemble techniques can be broadly categorized into two types:\n",
    "\n",
    "Bagging (Bootstrap Aggregating):\n",
    "\n",
    "Bagging involves training multiple instances of the same learning algorithm on different subsets of the training data, typically created by random sampling with replacement (bootstrap samples).\n",
    "The predictions of each model are then combined through averaging (for regression) or voting (for classification) to obtain the final prediction.\n",
    "Random Forest is a popular ensemble method based on bagging, where the base learners are decision trees.\n",
    "Boosting:\n",
    "\n",
    "Boosting focuses on building a sequence of weak learners (models that perform slightly better than random guessing) and combining their predictions in a weighted manner.\n",
    "Each model is trained to correct the errors of the previous one. Examples that are misclassified by earlier models are given higher weights, and subsequent models focus more on those examples.\n",
    "Gradient Boosting and AdaBoost are well-known boosting algorithms.\n",
    "Ensemble methods offer several advantages:\n",
    "\n",
    "Improved Generalization: Combining multiple models helps reduce overfitting and improves the model's ability to generalize to new, unseen data.\n",
    "\n",
    "Increased Stability: Ensemble methods are less sensitive to noise and outliers in the data, as errors made by individual models may be compensated by others.\n",
    "\n",
    "Enhanced Performance: Ensembles often outperform individual models, especially when the individual models have complementary strengths and weaknesses.\n",
    "\n",
    "Robustness: Ensembles are more robust in handling different types of data and variations in the training set.\n",
    "\n",
    "Popular ensemble methods include Random Forest, Gradient Boosting Machines (GBM), XGBoost, AdaBoost, and Stacking, among others. The choice of ensemble method depends on the characteristics of the data and the problem at hand.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3f5dfd-389f-4f40-b912-5eb4f93ad263",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab94e37-b990-481f-8da7-d16429098deb",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several reasons, and they offer various advantages that contribute to improved model performance and robustness. Here are some key reasons why ensemble techniques are widely employed:\n",
    "\n",
    "Increased Accuracy:\n",
    "\n",
    "Ensemble methods can significantly improve the accuracy of predictions by combining the strengths of multiple individual models. This is particularly beneficial when individual models may make errors on certain instances, but the ensemble can correct or mitigate those errors.\n",
    "Reduced Overfitting:\n",
    "\n",
    "Ensemble methods, especially bagging techniques like Random Forest, help reduce overfitting by averaging or combining predictions from multiple models. This is particularly useful when training complex models prone to overfitting.\n",
    "Improved Generalization:\n",
    "\n",
    "Ensemble methods enhance the generalization ability of models by leveraging diverse perspectives from multiple base learners. Each model may focus on different aspects of the data, leading to a more comprehensive understanding of the underlying patterns.\n",
    "Robustness to Noise:\n",
    "\n",
    "Ensembles are more robust to noisy data and outliers. The diversity among individual models allows the ensemble to be less influenced by individual errors or outliers, resulting in more robust predictions.\n",
    "Handling Complexity:\n",
    "\n",
    "In situations where the underlying relationship in the data is complex or non-linear, ensemble methods can capture intricate patterns by combining the predictive power of multiple models. This is especially important when dealing with high-dimensional or complex feature spaces.\n",
    "Versatility Across Algorithms:\n",
    "\n",
    "Ensemble techniques are algorithm-agnostic, meaning they can be applied to a variety of base learners. This allows practitioners to combine the strengths of different types of models (e.g., decision trees, support vector machines, neural networks) within the same ensemble.\n",
    "Flexibility in Design:\n",
    "\n",
    "Ensemble methods offer flexibility in design. Practitioners can choose different ensemble architectures, such as bagging or boosting, and experiment with various base learners to find the combination that works best for a specific problem.\n",
    "Incremental Learning:\n",
    "\n",
    "Boosting algorithms, in particular, facilitate incremental learning by sequentially improving the model with each iteration. This allows the model to adapt and improve over time.\n",
    "Handling Class Imbalance:\n",
    "\n",
    "Ensembles can help address class imbalance by assigning appropriate weights to different classes, making them useful for classification problems with uneven class distributions.\n",
    "State-of-the-Art Performance:\n",
    "\n",
    "Many state-of-the-art machine learning models and winners of machine learning competitions are based on ensemble methods, showcasing their effectiveness in real-world applications.\n",
    "In summary, ensemble techniques are used in machine learning to harness the benefits of diversity, reduce overfitting, improve accuracy, and create more robust models that generalize well to new, unseen data. Their versatility and performance make them a valuable tool in the machine learning practitioner's toolkit.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985a970f-de4d-49fe-8d98-83c15ab04d70",
   "metadata": {},
   "source": [
    "Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f6183d-1b2d-450d-99fb-8d283c8a3e5e",
   "metadata": {},
   "source": [
    "Bagging, which stands for Bootstrap Aggregating, is an ensemble technique in machine learning that aims to improve the stability and accuracy of a model by training multiple instances of the same learning algorithm on different subsets of the training data. The key idea behind bagging is to introduce diversity among the base learners by training them on random samples of the dataset.\n",
    "\n",
    "The bagging process involves the following steps:\n",
    "\n",
    "Bootstrap Sampling:\n",
    "\n",
    "Randomly draw multiple subsets (samples) from the original training dataset with replacement. Each subset is of the same size as the original dataset but may contain duplicate instances due to the sampling with replacement.\n",
    "Model Training:\n",
    "\n",
    "Train a base learner (typically the same learning algorithm) on each of the bootstrap samples. Since the samples are different, each base learner is exposed to a slightly different subset of the data.\n",
    "Prediction Aggregation:\n",
    "\n",
    "Combine the predictions of individual models to obtain the final prediction. The aggregation process depends on the type of problem:\n",
    "For regression problems, predictions are often averaged.\n",
    "For classification problems, a majority vote is typically used.\n",
    "The key benefits of bagging include:\n",
    "\n",
    "Reduction of Variance: By training models on different subsets of the data, bagging helps reduce the variance of the model. This is particularly useful when dealing with complex models that may overfit to specific patterns in the training data.\n",
    "\n",
    "Improved Stability: Bagging makes the model more robust to outliers and noise in the data, as the impact of individual instances is diminished by the diversity introduced through sampling.\n",
    "\n",
    "Enhanced Generalization: The ensemble's ability to generalize to new, unseen data is often improved compared to individual models, leading to better overall performance.\n",
    "\n",
    "Parallelization: The independent nature of training individual models makes bagging amenable to parallelization, allowing for faster training on distributed computing resources.\n",
    "\n",
    "A prominent example of a bagging algorithm is the Random Forest, which employs bagging with decision trees as base learners. In Random Forest, each tree is trained on a different bootstrap sample, and the final prediction is obtained by aggregating the predictions of all trees.\n",
    "\n",
    "Bagging is a versatile and effective technique, and its application is not limited to decision trees. It can be used with various base learners, provided they can handle multiple subsets of the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce6f787-d2a1-454e-8c39-b9a4b28db66b",
   "metadata": {},
   "source": [
    "Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a4bbdf-6e83-4f4b-ba72-9ed25fed5648",
   "metadata": {},
   "source": [
    "Boosting is an ensemble technique in machine learning that aims to improve the performance of a model by combining the predictions of multiple weak learners (models that perform slightly better than random guessing). The key idea behind boosting is to sequentially train weak models, giving more emphasis to instances that were misclassified by the previous models. This iterative process focuses on improving the model's performance on challenging instances, ultimately creating a strong and accurate ensemble model.\n",
    "\n",
    "The boosting process typically involves the following steps:\n",
    "\n",
    "Sequential Training:\n",
    "\n",
    "Train a base learner (weak model) on the original training data.\n",
    "Instance Weighting:\n",
    "\n",
    "Assign weights to instances in the training data. Initially, all instances have equal weights.\n",
    "Weighted Training:\n",
    "\n",
    "Train the base learner on the weighted training data. The model focuses more on instances with higher weights, i.e., instances that were misclassified by the previous models.\n",
    "Weight Adjustment:\n",
    "\n",
    "Increase the weights of misclassified instances, making them more influential in the next iteration. This gives subsequent models a higher emphasis on correcting errors.\n",
    "Iterative Process:\n",
    "\n",
    "Repeat the process for a predefined number of iterations or until a stopping criterion is met. Each new model focuses on correcting the mistakes of the ensemble formed by the previous models.\n",
    "Final Aggregation:\n",
    "\n",
    "Combine the predictions of all base learners, often using weighted voting, to obtain the final prediction.\n",
    "The key characteristics and benefits of boosting include:\n",
    "\n",
    "Sequential Learning: Boosting builds a sequence of models where each model corrects the errors of the previous ones. This makes boosting particularly effective in handling complex relationships in the data.\n",
    "\n",
    "Emphasis on Misclassified Instances: Boosting assigns higher importance to instances that are challenging for the current ensemble, leading to a strong emphasis on difficult-to-classify cases.\n",
    "\n",
    "Reduced Bias and Variance: Boosting aims to reduce both bias and variance, making it capable of achieving high accuracy on a variety of tasks.\n",
    "\n",
    "Adaptive Learning: The model adapts to the complexities of the data over iterations, potentially capturing intricate patterns that might be overlooked by a single model.\n",
    "\n",
    "Versatility: Boosting can be applied to a variety of base learners, making it versatile across different types of models.\n",
    "\n",
    "Common algorithms based on boosting include AdaBoost (Adaptive Boosting), Gradient Boosting Machines (GBM), and XGBoost (eXtreme Gradient Boosting).\n",
    "\n",
    "Boosting is a powerful technique in machine learning, and it often outperforms individual models and other ensemble methods. However, it is more sensitive to noisy data and outliers compared to bagging techniques like Random Forest.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ef0aee-53fd-487e-b24d-46d6552fb780",
   "metadata": {},
   "source": [
    "Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9585da-c9ca-4100-93d4-0e2b94ea67ff",
   "metadata": {},
   "source": [
    "Ensemble techniques offer several benefits in machine learning, contributing to improved model performance and robustness. Here are key advantages of using ensemble techniques:\n",
    "\n",
    "Improved Accuracy:\n",
    "\n",
    "Ensemble methods often result in higher accuracy compared to individual models. Combining the predictions of multiple models helps mitigate the impact of errors made by individual models and leverages the strengths of different models.\n",
    "Reduction of Overfitting:\n",
    "\n",
    "Ensemble techniques, especially bagging, help reduce overfitting by averaging or combining predictions from multiple models. This is particularly beneficial when individual models are prone to overfitting the training data.\n",
    "Enhanced Generalization:\n",
    "\n",
    "Ensembles generalize well to new, unseen data. The diversity among the individual models allows the ensemble to capture a broader range of patterns in the data, leading to better generalization.\n",
    "Robustness to Noise and Outliers:\n",
    "\n",
    "Ensembles are more robust to noisy data and outliers. The impact of individual errors or outliers is diminished when combining predictions from multiple models, resulting in more robust and stable predictions.\n",
    "Versatility Across Algorithms:\n",
    "\n",
    "Ensemble methods are algorithm-agnostic, meaning they can be applied to a variety of base learners. This allows practitioners to combine the strengths of different types of models within the same ensemble.\n",
    "Flexibility in Model Design:\n",
    "\n",
    "Ensembles provide flexibility in model design. Practitioners can experiment with different ensemble architectures (bagging, boosting, stacking) and various base learners to find the combination that works best for a specific problem.\n",
    "Increased Stability:\n",
    "\n",
    "Ensembles are less sensitive to variations in the training data, as the diversity among models helps maintain stability. This is particularly valuable when dealing with small or noisy datasets.\n",
    "Parallelization and Scalability:\n",
    "\n",
    "Many ensemble methods, especially bagging, are amenable to parallelization. This allows for faster training on distributed computing resources, making ensembles scalable to large datasets.\n",
    "State-of-the-Art Performance:\n",
    "\n",
    "Ensembles, especially those based on boosting algorithms, have demonstrated state-of-the-art performance in various machine learning competitions and real-world applications.\n",
    "Incremental Learning:\n",
    "\n",
    "Boosting algorithms facilitate incremental learning by sequentially improving the model with each iteration. This adaptability is useful when dealing with evolving data or changing patterns over time.\n",
    "Handling Class Imbalance:\n",
    "\n",
    "Ensembles can help address class imbalance by adjusting the weights assigned to different classes, making them suitable for classification problems with uneven class distributions.\n",
    "Complementary Strengths:\n",
    "\n",
    "Ensemble methods leverage the complementary strengths and weaknesses of individual models, resulting in a more comprehensive and accurate model.\n",
    "In summary, ensemble techniques offer a powerful approach to improving model performance by combining the predictive abilities of multiple models. Their ability to handle diverse data characteristics, reduce overfitting, and improve generalization makes them a valuable tool in the machine learning practitioner's toolkit.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204aded7-d937-4b8c-9c4e-b57dbe1f680d",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0be0af-34f5-4843-8d9a-6901c73a31a8",
   "metadata": {},
   "source": [
    "While ensemble techniques often outperform individual models and are widely used in practice, it is not an absolute rule that ensembles are always better. The effectiveness of ensemble techniques depends on several factors, and there are scenarios where using an ensemble might not lead to significant improvements or might even be detrimental. Here are some considerations:\n",
    "\n",
    "Data Size and Complexity:\n",
    "\n",
    "In cases where the dataset is small or the underlying patterns are simple, using an ensemble may not provide significant benefits. Individual models might already capture the available information adequately.\n",
    "Computational Resources:\n",
    "\n",
    "Ensemble techniques, especially those involving a large number of models or boosting iterations, can be computationally expensive. In situations where computational resources are limited, training and maintaining an ensemble may not be practical.\n",
    "Data Quality:\n",
    "\n",
    "If the dataset is of low quality, contains significant noise, or has unreliable labels, ensemble methods might propagate errors and noise. In such cases, improving data quality may have a more substantial impact.\n",
    "Overfitting:\n",
    "\n",
    "Ensembles can still be susceptible to overfitting, especially if individual models are overly complex or the ensemble is too large. Regularization techniques and careful model tuning are necessary to prevent overfitting in ensembles.\n",
    "Model Diversity:\n",
    "\n",
    "The success of an ensemble often relies on the diversity among individual models. If the base learners are too similar, the ensemble might not gain the benefits of combining diverse perspectives. Ensuring diversity among base models is crucial for the effectiveness of ensembles.\n",
    "Problem Characteristics:\n",
    "\n",
    "The nature of the problem itself can influence the effectiveness of ensembles. In some cases, problems may be inherently simple, and adding complexity through ensembles might not be necessary.\n",
    "Interpretability:\n",
    "\n",
    "Ensembles, particularly those with a large number of models, can be challenging to interpret. If interpretability is a critical requirement, a simpler individual model might be preferred.\n",
    "Training Time:\n",
    "\n",
    "In real-time or near-real-time applications, the training time of ensembles may be a limitation. In such cases, individual models that can be trained quickly may be favored.\n",
    "Resource Constraints:\n",
    "\n",
    "In resource-constrained environments, deploying and maintaining an ensemble of models may be impractical. A single, well-tuned model might be more feasible.\n",
    "In summary, while ensemble techniques are powerful and often lead to improved performance, it's important to consider the specific characteristics of the problem, the quality of the data, and the available resources. It's advisable to experiment with both individual models and ensembles, considering factors such as interpretability, computational cost, and the complexity of the problem at hand. The choice between using an ensemble or an individual model should be guided by empirical results and a thorough understanding of the specific requirements and constraints of the problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c9d437-8255-45de-b3d8-2f9e00a16d1d",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432e35e4-79e4-4971-9419-3cc741844783",
   "metadata": {},
   "source": [
    "The confidence interval using bootstrap resampling involves repeatedly sampling from the observed data with replacement to create multiple bootstrap samples. From these samples, statistics are calculated, and the distribution of the statistic is used to estimate the confidence interval. The process can be summarized in the following steps:\n",
    "\n",
    "Data Resampling:\n",
    "\n",
    "Randomly draw \n",
    "�\n",
    "B bootstrap samples with replacement from the observed dataset. Each bootstrap sample has the same size as the original dataset.\n",
    "Statistic Calculation:\n",
    "\n",
    "For each bootstrap sample, calculate the statistic of interest (e.g., mean, median, standard deviation, etc.).\n",
    "Bootstrap Distribution:\n",
    "\n",
    "Form a distribution of the calculated statistic across all bootstrap samples.\n",
    "Confidence Interval Estimation:\n",
    "\n",
    "Determine the lower and upper bounds of the confidence interval based on the desired level of confidence (e.g., 95%, 99%). This is done by finding the quantiles of the bootstrap distribution.\n",
    "The formula for a confidence interval using bootstrap involves finding the appropriate percentiles of the bootstrap distribution. Let's denote the lower percentile as \n",
    "�\n",
    "L and the upper percentile as \n",
    "�\n",
    "U. For a \n",
    "95\n",
    "%\n",
    "95% confidence interval, \n",
    "�\n",
    "=\n",
    "2.5\n",
    "%\n",
    "L=2.5% and \n",
    "�\n",
    "=\n",
    "97.5\n",
    "%\n",
    "U=97.5% (assuming a symmetric interval). The confidence interval is then given by:\n",
    "\n",
    "Confidence Interval\n",
    "=\n",
    "[\n",
    "Percentile\n",
    "(\n",
    "�\n",
    ")\n",
    ",\n",
    "Percentile\n",
    "(\n",
    "�\n",
    ")\n",
    "]\n",
    "Confidence Interval=[Percentile(L),Percentile(U)]\n",
    "\n",
    "Here's a step-by-step breakdown:\n",
    "\n",
    "Sort the bootstrap distribution in ascending order.\n",
    "\n",
    "Find the value at the \n",
    "�\n",
    "L-th percentile (e.g., \n",
    "2.5\n",
    "%\n",
    "2.5%) to get the lower bound.\n",
    "\n",
    "Find the value at the \n",
    "�\n",
    "U-th percentile (e.g., \n",
    "97.5\n",
    "%\n",
    "97.5%) to get the upper bound.\n",
    "\n",
    "The result is a confidence interval that provides an estimate of the range in which the true population parameter is likely to lie.\n",
    "\n",
    "It's important to note that the bootstrap method assumes that the observed data is representative of the population and that the underlying assumptions of the statistical analysis are met. Additionally, the accuracy of the confidence interval may depend on the number of bootstrap samples (\n",
    "�\n",
    "B), with larger values typically leading to more accurate estimates.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba386658-cb2b-419c-acdf-2653c6218e9b",
   "metadata": {},
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb51f674-5337-487d-be35-dfa2a263cd3e",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique used in statistics to estimate the variability and uncertainty associated with a sample statistic by repeatedly resampling with replacement from the observed data. The key idea is to create multiple bootstrap samples that mimic the variability present in the original dataset. The steps involved in the bootstrap process can be summarized as follows:\n",
    "\n",
    "Original Dataset:\n",
    "\n",
    "Begin with an observed dataset of size \n",
    "�\n",
    "n, where \n",
    "�\n",
    "n is the number of data points.\n",
    "Resampling with Replacement:\n",
    "\n",
    "Draw \n",
    "�\n",
    "B bootstrap samples from the observed dataset by randomly selecting \n",
    "�\n",
    "n data points with replacement for each sample. This means that some data points may be repeated in a given bootstrap sample, while others may be omitted.\n",
    "Statistic Calculation:\n",
    "\n",
    "For each bootstrap sample, calculate the statistic of interest. This could be the mean, median, standard deviation, regression coefficients, or any other measure that you want to estimate.\n",
    "Bootstrap Distribution:\n",
    "\n",
    "Collect the calculated statistics from all \n",
    "�\n",
    "B bootstrap samples to form the bootstrap distribution of the statistic.\n",
    "Variability and Confidence Intervals:\n",
    "\n",
    "Assess the variability of the statistic by examining the spread of values in the bootstrap distribution. Construct confidence intervals by finding the appropriate percentiles of the distribution.\n",
    "Statistical Inference:\n",
    "\n",
    "Use the bootstrap distribution to make statistical inferences, such as estimating confidence intervals, standard errors, and hypothesis testing.\n",
    "The fundamental concept behind bootstrap is that the distribution of the sample statistic from the resampled data approximates the distribution of the statistic in the population. By creating multiple bootstrap samples, you obtain an empirical estimate of the sampling distribution of the statistic without assuming a specific parametric distribution.\n",
    "\n",
    "The benefits of bootstrap include its simplicity, versatility, and applicability to a wide range of statistical problems. It is particularly useful when the underlying distribution is unknown or when the sample size is small. However, bootstrap results may be sensitive to the characteristics of the original dataset, and care must be taken to ensure that the assumptions of the statistical analysis are met.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19365d89-bb25-44f4-b286-9d6d60235697",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df297ba9-ed5a-4cfb-9b03-119215f549ce",
   "metadata": {},
   "source": [
    "To estimate the 95% confidence interval for the population mean height using bootstrap, we'll follow these steps:\n",
    "\n",
    "Original Data:\n",
    "\n",
    "Start with the original sample of 50 tree heights.\n",
    "Resampling with Replacement:\n",
    "\n",
    "Draw multiple bootstrap samples with replacement from the original sample. The number of bootstrap samples (B) is chosen based on the desired precision, but common choices are in the range of 1000 to 10,000.\n",
    "Statistic Calculation:\n",
    "\n",
    "For each bootstrap sample, calculate the mean height.\n",
    "Bootstrap Distribution:\n",
    "\n",
    "Form a distribution of the mean heights from all bootstrap samples.\n",
    "Confidence Interval Estimation:\n",
    "\n",
    "Determine the lower and upper bounds of the 95% confidence interval based on the percentiles of the bootstrap distribution.\n",
    "Now, let's perform the calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "866e1c95-3fe4-405c-8fe0-cff02d930e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for the Population Mean Height: [14.50151008 15.55054176]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "original_sample = np.random.normal(loc=15, scale=2, size=50)  \n",
    "num_bootstrap_samples = 10000\n",
    "\n",
    "\n",
    "bootstrap_samples = np.random.choice(original_sample, (num_bootstrap_samples, len(original_sample)), replace=True)\n",
    "\n",
    "\n",
    "bootstrap_means = np.mean(bootstrap_samples, axis=1)\n",
    "\n",
    "\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(\"95% Confidence Interval for the Population Mean Height:\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a69803-0478-4cc9-b6d7-b6d07eb5d0ca",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "We generate a random normal distribution to simulate the original sample.\n",
    "We then perform the bootstrap resampling, calculating the mean for each bootstrap sample.\n",
    "Finally, we find the 2.5th and 97.5th percentiles of the bootstrap distribution to construct the 95% confidence interval.\n",
    "Keep in mind that the actual implementation may vary depending on the programming language or tool you're using. The key is to perform resampling with replacement and calculate the desired statistic (mean, in this case) for each bootstrap sample to create the distribution from which the confidence interval is derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d9ab8-6a39-4877-897e-03eb51712288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
